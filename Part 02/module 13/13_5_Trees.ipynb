{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wR1F4tBh7qoe"
      },
      "source": [
        "# Курс Data Science\n",
        "## Модуль 13.5 Деревья решений.\n",
        "\n",
        "Деревья решений - один из наиболее популярных методов классификации. Одной из причин их популярности является то, что окончательную модель крайне легко понять - достаточно построить граф решений и посмотреть, почему был сделан тот или иной прогноз.\n",
        "$$ $$\n",
        "Также деревья решений являются основой таких методов как бэггинг, случайные леса и градиентный бустинг, о которых будем говорить позднее."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RLJSQb3ACkd3"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fjZCvDynCkuR"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V3Bl6YT77qof"
      },
      "source": [
        "Например, деревья решений часто используются в банковском скоринге (системе оценки клиентов). Наглядно решение о выдаче кредита заемщику можно представить в таком виде:\n",
        "\n",
        "![](https://248006.selcdn.ru/public/DS_Block2_M5_final/DT.gif)\n",
        "\n",
        "По сути, деревьями принятия решений решается задача бинарной классификации (здесь листы решающего дерева могут иметь только два значения: \"Выдать кредит\" и \"Отказать\"). Как видно по графу, огромное преимущество деревьев состоит в том, что они легко интерпретируемы: например, в этом примере по схеме можно объяснить, почему было принято решение об отказе в кредите."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S38R-HZK7qov"
      },
      "source": [
        "Перейдем к алгоритму деревьев решений, реализованному в библиотеке *sklearn* - CART. Его можно использовать как для задач классификации, так и для задач регрессии. Мы сосредоточимся на задаче классификации.\n",
        "\n",
        "Модель CART представляет собой двоичное дерево - каждый узел может иметь нуль, один или два дочерних узла.\n",
        "\n",
        "Узел представляет собой точку разделения входных данных, а конечный (терминальный) узел - выходную переменную, которая будет использоваться для прогнозирования.\n",
        "\n",
        "Посмотрим, как строится такое дерево на упрощенном примере, показывающем выживаемость пассажиров корабля Титаник на основе возраста (age), пола (sex) и наличия супруга и братьев и сестер (sibsp)  на борту.\n",
        "\n",
        "![](https://248006.selcdn.ru/public/DS_Block2_M5_final/CART_titanic.png)\n",
        "\n",
        "Десятичные числа под листьями показывают вероятность выживания, а целые - процент наблюдений (от общего количества пассажиров) в листе."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NjYW777n7qoh"
      },
      "source": [
        "Если со структурой дерева все понятно, то как понять, каким образом лучше разделить выборку данных? Ведь у нас не всегда бинарные признаки  типа \"имеется дом\" или \"не имеется\" - в этом наборе данных, например, есть непрерывная переменная \"возраст\". В этом случае помогут определенные критерии качества разбиения. Обычно в задаче классификации используются энтропия или неопределенность Джини."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QHTGFaQX7qop"
      },
      "source": [
        "По своей сути *энтропия* - это степень хаоса в системе: чем больше различных элементов в множестве, тем выше его энтропия.\n",
        "В целом, алгоритм подбирает такое разбиение, чтобы минимизировать энтропию, т.е. уменьшить количество объектов разных классов в листе.\n",
        "\n",
        "Подробнее об энтропии в деревьях принятия решений и представленном примере [здесь](https://habr.com/ru/post/171759/)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WoUomWSv7qoq"
      },
      "source": [
        "Неопределенность Джини можно интерпретировать как вероятность неправильной классификации, если предсказывать классы с вероятностями их появления в этом узле. Соответственно, чем меньше неопределенность Джини, тем более точно работает модель.\n",
        "\n",
        "На деле минимизация энтропии и неопределенность Джини работают почти одинаково - график энтропии очень близок к графику удвоенной неопределенности Джини, поэтому разница в их использовании будет минимальна."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JiM-peX-kB3i"
      },
      "source": [
        "Теперь вернемся к нашему примеру. Для начала по какому-то заранее определенному критерию (например, энтропии) алгоритм выбирает *начальную точку разбиения входных данных*, рассчитав этот критерий для каждого признака - получает разделение по половому признаку, что очевидно, т.к. разделение по бинарному признаку в самом начале (определение пола) даст больше информации, чем перебор всех возрастов. Переходим к 3-му пункту алгоритма построения дерева - рекурсивному повторению вычислений для каждого получившегося листа.\n",
        "\n",
        "Далее для одного листа заканчиваем вычисления (возможно, это конечный узел - мы получили нужную вероятность, поэтому дальше вычисления не имеют смысла), а для второго листа вычисляем критерий качества либо для всех возрастов, либо разделяем их на определенные промежутки (например, считаем энтропию через каждые два года, начиная с самого раннего возраста, что упрощает вычисления). Получаем, что лучшее разбиение по критерию качества дает нам *точку разделения данных* для следующего листа 9,5 лет. Далее рекурсивно продолжаем вычисления, пока не получим необходимую информацию.\n",
        "\n",
        "Подводя итог: шансы на выживание у пассажира были бы больше, если бы он был либо женщиной, либо маленьким мальчиков без большого количества родственников на борту."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z7wAMUbP7qor"
      },
      "source": [
        "В общем виде алгоритм построения дерева можно описать следующим образом:\n",
        "- вычисляем выбранный критерий качества разбиения для всех элементов исходного множества\n",
        "- выбираем такое разбиение, при котором критерий качества является наилучшим\n",
        "- рекурсивно повторяем процедуру для каждого получившегося подмножества\n",
        "\n",
        "Критерием останова может служить как *энтропия или неопределенность Джини == 0*, так и заранее выбранная максимальная глубина дерева, максимальное количество листов дерева и т.д."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9AaOQDtk7qox"
      },
      "source": [
        "Модель предоставляет возможность варьировать различные гиперпараметры алгоритма, такие как критерий разбиения, максимальная глубина дерева, стратегия разбиения на поддеревья, минимальное количество объектов в листе и т.д. \n",
        "\n",
        "Когда мы строим деревья решений, существует большой риск переобучения, так как дерево можно разбить на сколько угодно поддеревьев, которые способны разделять обучающий набор данных вплоть до листов, в которых будет всего один элемент - для предсказаний такие деревья почти бесполезны, так как они слишком точно описывают один тестовый набор данных, в то время как другие данные, которые необходимо классифицировать, могут сильно от него отличаться. В целом, не существует одного правила, которое бы описывало построение любого дерева - в каждом случае критерий останова подбирается индивидуально. Однако можно предложить несколько эвристических правил:\n",
        "- заранее определенная максимальная глубина дерева (количество узлов одного уровня в дереве - по сути, количество ветвей от первого узла до листа) чревата тем, что будут пропущены важные, но труднообнаруживаемые разбиения, поэтому лучше использовать последующую обрезку дерева, когда сначала строится все дерево решений, а потом по определенным правилам производится его обрезка (например, отсекаются те узлы и ветви, использование которых мало влияет на качество классификации - не сильно уменьшает ошибки);\n",
        "- разбиение должно быть нетривиальным, т.е. полученные в результате узлы должны сорержать не менее заданного количества объектов - как мы уже отметили,  переобученная на тренировочных данных модель не имеет практической пользы для классификации новых данных;\n",
        "- если даже усеченные (после обрезки) деревья все еще сложны для восприятия, можно прибегнуть к методике извлечения правил (условий в узлах) из дерева с последующим созданием наборов правил в одном узле.\n",
        "\n",
        "Мы же ограничимся тем, что построим две модели с критериями качества разбиения, которые рассматривали выше."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Gcj-16Q7qoy"
      },
      "source": [
        "По умолчанию алгоритм использует неопределенность Джини."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mjtsANSN7qo0"
      },
      "outputs": [],
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "dtc = DecisionTreeClassifier()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 164
        },
        "id": "BIBOCQLQ7qo7",
        "outputId": "2efb53cd-9fbe-4366-9694-393b2e56a969"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-21d0bcfe5b15>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdtc_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdtc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'x_train' is not defined"
          ]
        }
      ],
      "source": [
        "dtc_model = dtc.fit(x_train, y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_YN7zax17qpB"
      },
      "source": [
        "Визуализируем граф обученной модели. Для этого понадобится установить библиотеку *graphviz*."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UMadvXnq7qpC"
      },
      "outputs": [],
      "source": [
        "from sklearn import tree\n",
        "import graphviz\n",
        "\n",
        "def print_graph(data):\n",
        "    dot_data = tree.export_graphviz(data, out_file=None,\n",
        "                                    feature_names=iris_dataset.feature_names[2:4],  \n",
        "                                    class_names=iris_dataset.target_names,  \n",
        "                                    filled=True)  \n",
        "    return graphviz.Source(dot_data)  \n",
        "\n",
        "print_graph(dtc_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3_2jCveA7qpH"
      },
      "source": [
        "Такое дерево читается по цветам: чем больше в узле объектов одного класса, тем насыщеннее будет его цвет: как мы видим, при соотношении объектов внутри узла (50:50), цвет узла является чисто белым, а при преобладании объектов какого-либо из классов, он становится ближе к цвету того класса, объектов которого в узле больше. В данном случае почти во всех узлах преобладают цветки класса \"virginica\".\n",
        "\n",
        "Но вообще, больше информации мы можем получить, посмотрев на условия разбиения в каждом узле (условия вида x < a)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jS4wY4HK7qpI"
      },
      "source": [
        "Получим предсказания для тестовых данных."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xj_GevnY7qpK"
      },
      "outputs": [],
      "source": [
        "dtc_predictions = dtc.predict(x_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "meWaaDiD7qpM"
      },
      "source": [
        "Для определения точности предсказаний воспользуемся встроенной функцией score."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h2Ro-bQ77qpN"
      },
      "outputs": [],
      "source": [
        "accuracy = dtc.score(x_test, y_test)\n",
        "print(f'Accuracy: {accuracy}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1vybryoD7qpP"
      },
      "source": [
        "Теперь построим и обучим модель с критерием качества разбиения *энтропия*."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JgFdGPTf7qpQ"
      },
      "outputs": [],
      "source": [
        "dtc_entrp = DecisionTreeClassifier(criterion='entropy')\n",
        "dtc_model_entrp = dtc_entrp.fit(x_train, y_train)\n",
        "dtc_predictions_entrp = dtc_entrp.predict(x_test)\n",
        "accuracy = dtc_entrp.score(x_test, y_test)\n",
        "print(f'Accuracy: {accuracy}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aPdT4c3P7qpT"
      },
      "outputs": [],
      "source": [
        "print_graph(dtc_model_entrp)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-D746Pwe7qpZ"
      },
      "source": [
        "Как мы можем видеть, при таких параметрах алгоритм работает менее точно, чем kNN и наивный Байес. Можно предположить, что алгоритму не хватает двух признаков для качественного разбиения на поддеревья - на большем количестве данных по каждому объекту метод скорее всего будет работать более точно."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u7O_xQkX7qpa"
      },
      "source": [
        "Определенно, деревья решений имеют ряд плюсов: они легко интерпретируемы, визуализируемы (исчезает эффект \"черного ящика\"), достаточно быстро обучаются и делают прогнозы, имеют небольшое количество параметров модели и поддерживают как категориальные, так и числовые признаки. \n",
        "\n",
        "Однако при этом они очень чувствительны к шумам во входных данных, подвержены переобучению - для борьбы с ним необходимо корректно устанавливать гиперпараметры (максимальную глубину дерева или минимальное число элементов в листьях деревьев), а также не умеют предсказывать данные, выходящие за рамки обучающего датасета"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.9.1 ('ds_env': venv)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.1"
    },
    "vscode": {
      "interpreter": {
        "hash": "248bc044bc391cd70479aa3aa6b95972b092e756825b5eb21a1dc6ccdc62151a"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
